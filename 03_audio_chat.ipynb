{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f5b292",
   "metadata": {},
   "source": [
    "Load libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39a2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from agents import (\n",
    "  Agent, \n",
    "  handoff, \n",
    "  RunContextWrapper, \n",
    "  Runner, \n",
    "  AsyncOpenAI, \n",
    "  set_default_openai_client, \n",
    "  set_default_openai_api,\n",
    "  function_tool\n",
    ")\n",
    "\n",
    "from agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e25843",
   "metadata": {},
   "source": [
    "Load Environments and set to use Azure Open AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c40ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "\n",
    "OPEN_WEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "\n",
    "OPENAPI_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "external_client = AsyncOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT_NAME}\",\n",
    "    default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    default_query={\"api-version\": AZURE_OPENAI_API_VERSION}\n",
    ")\n",
    "\n",
    "transcript_client = AsyncOpenAI(\n",
    "    api_key=OPENAPI_KEY,\n",
    ")\n",
    "\n",
    "set_default_openai_client(external_client, use_for_tracing=False)\n",
    "set_default_openai_api(\"chat_completions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c57755",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"\"\"\n",
    "You are a sales agent. Given a specific prompt, you extract the intend \\\n",
    "of a prompt. You only respond to the intent related to sales quotation. \\\n",
    "If the intent does not relate to the sales quotation, you reject the prompt. \\\n",
    "When the intent is requesting the sales quotation, you must use get_quotation tool to \\\n",
    "retrieve the sales quotation. \\\n",
    "When the sales quotation is available, display both the intent of the prompt and the quotation. \\\n",
    "Otherwise, you display the intent of the prompt. \\\n",
    "\n",
    "When you reject the prompt, you should tell the REASON of rejecting the prompt. \\\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@function_tool\n",
    "async def get_quotation():\n",
    "  with open('sample-quotation/soq-1.json') as f:\n",
    "    data = json.load(f)\n",
    "  return f\"Here is the content of the quotation:\\n\\n{json.dumps(data, indent=2)}\\n\"\n",
    "\n",
    "sales_agent = Agent(\n",
    "  name = \"Sales Agent\",\n",
    "  instructions=INSTRUCTIONS,\n",
    "  tools=[get_quotation]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b675d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def transcribe(audio_path):\n",
    "  with open(audio_path, 'rb') as audio_file:\n",
    "    transcript = await transcript_client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\",\n",
    "      # model = \"gpt-4o-mini-transcribe\",\n",
    "      file=(audio_path, audio_file, \"audio/wav\"),\n",
    "      response_format=\"text\"\n",
    "    )\n",
    "  print(f\"Transcription: {transcript}\")\n",
    "  return transcript\n",
    "\n",
    "async def run_agent_with_transcript(transcript: str):\n",
    "  result = await Runner.run(sales_agent, transcript)\n",
    "  return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450c51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def transcribe_and_route(audio_path):\n",
    "  text = await transcribe(audio_path)\n",
    "  response = await run_agent_with_transcript(text)\n",
    "  return response\n",
    "\n",
    "def user_msg(msg: str):\n",
    "  return f\"\"\"\n",
    "  <div style=\"text-align: right;\">\n",
    "    Me: {msg}\n",
    "  </div>\n",
    "  \"\"\"\n",
    "\n",
    "def bot_msg(msg: str):\n",
    "  return f\"\"\"\n",
    "  <div style=\"text-align: left;\">\n",
    "      <span style=\"display: inline-block; padding: 8px 12px; border-radius: 16px; max-width: 70%;\">\n",
    "          Bot: <strong>{msg}</strong>\n",
    "      </span>\n",
    "  </div>\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "async def chat_wrapper(audio_path, history):\n",
    "  transcription = await transcribe(audio_path)\n",
    "  history.append((\"User\", user_msg(transcription)))\n",
    "\n",
    "  response = await run_agent_with_transcript(transcription)\n",
    "  history.append((\"Bot\", bot_msg(response)))\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b466f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/dbcl9py57hd2kv4rqh70f6n40000gn/T/ipykernel_14468/2169899381.py:17: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Tell me how is the temperature now?\n",
      "\n",
      "Transcription: Then can you show me the sales quotation?\n",
      "\n",
      "Transcription: Ok, can you suggest me one of the famous restaurants in Puchong?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# gr.Interface(\n",
    "#     fn=transcribe_and_route,\n",
    "#     inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "#     outputs=[\"text\"],\n",
    "#     title=\"Voice Intent Agent\",\n",
    "#     description=\"Uses Whisper for transcription and Agent SDK for intent recognition.\"\n",
    "# ).launch()\n",
    "\n",
    "def process_audio(audio_path, chat_history):\n",
    "    return asyncio.run(chat_wrapper(audio_path, chat_history))\n",
    "\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "  gr.Markdown('Uses Whisper for transcription and Agent SDK for intent recognition.')\n",
    "  chatbot = gr.Chatbot()\n",
    "  audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "  btn = gr.Button(\"Submit\")\n",
    "\n",
    "  state = gr.State([])\n",
    "\n",
    "  btn.click(fn=process_audio, inputs=[audio, state], outputs=chatbot).then(\n",
    "        fn=lambda x: x, inputs=chatbot, outputs=state\n",
    "    ).then(\n",
    "       fn=lambda: None, inputs=None, outputs=audio\n",
    "    )\n",
    "\n",
    "app.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
